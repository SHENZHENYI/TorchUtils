{
 "cells": [
  {
   "source": [
    "This is a example training script for image classification using TorchUtils.\n",
    "The dataset and the task are introduced by Mu Li, at [Kaggle](https://www.kaggle.com/c/classify-leaves)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "august-tolerance",
   "metadata": {},
   "source": [
    "## Prepare Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, you should install TorchUtils (see README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "medical-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn import metrics\n",
    "import urllib\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import random\n",
    "import sys\n",
    "import gc\n",
    "import shutil\n",
    "from tqdm.autonotebook import tqdm\n",
    "import albumentations\n",
    "from albumentations import pytorch as AT\n",
    "\n",
    "import scipy.special\n",
    "sigmoid = lambda x: scipy.special.expit(x)\n",
    "from scipy.special import softmax\n",
    "\n",
    "import torch_utils as tu \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clean-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "base_dir = '../input/'\n",
    "tu.tools.seed_everything(SEED, deterministic=False)\n",
    "tu.tools.set_gpus('0,1') # gpu ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed77469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP = 1\n",
    "while os.path.exists('../exp/exp%d'%EXP):\n",
    "    EXP+=1\n",
    "os.makedirs('../exp/exp%d'%EXP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-integration",
   "metadata": {},
   "source": [
    "## Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uniform-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 176\n",
    "FOLD = 5\n",
    "BATCH_SIZE = 64\n",
    "ACCUMULATE = 1\n",
    "LR = 3e-4\n",
    "EPOCH = 36\n",
    "DECAY_SCALE = 20.0\n",
    "MIXUP = 0 # 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-joshua",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exact-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = albumentations.Compose([\n",
    "    albumentations.RandomRotate90(p=0.5),\n",
    "    albumentations.Transpose(p=0.5),\n",
    "    albumentations.Flip(p=0.5),\n",
    "    albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.0625, rotate_limit=45, border_mode=1, p=0.5),\n",
    "    tu.randAugment(),\n",
    "    albumentations.Normalize(),\n",
    "    AT.ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "test_transform = albumentations.Compose([\n",
    "    albumentations.Normalize(),\n",
    "    AT.ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "class LeavesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, label_encoder, data_path='../input', transform = train_transform): \n",
    "        self.df = df \n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.df.label = self.df.label.apply(lambda x: label_encoder[x])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.df.image[idx], self.df.label[idx]\n",
    "        img_path = os.path.join(self.data_path, img_path)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transform(image = img)['image']\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "loaded-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(base_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "typical-humanity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/0.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/1.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/2.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/3.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/4.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image             label\n",
       "0  images/0.jpg  maclura_pomifera\n",
       "1  images/1.jpg  maclura_pomifera\n",
       "2  images/2.jpg  maclura_pomifera\n",
       "3  images/3.jpg  maclura_pomifera\n",
       "4  images/4.jpg  maclura_pomifera"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "needed-anxiety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14682 3671\n",
      "14682 3671\n",
      "14682 3671\n",
      "14683 3670\n",
      "14683 3670\n"
     ]
    }
   ],
   "source": [
    "sfolder = StratifiedKFold(n_splits=FOLD,random_state=SEED,shuffle=True)\n",
    "tr_folds = []\n",
    "val_folds = []\n",
    "for train_idx, val_idx in sfolder.split(train_df.image, train_df.label):\n",
    "    tr_folds.append(train_idx)\n",
    "    val_folds.append(val_idx)\n",
    "    print(len(train_idx), len(val_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-kennedy",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worse-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scaler = torch.cuda.amp.GradScaler() # for AMP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "latest-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, verbose=False):\n",
    "    model_conv.train()         \n",
    "    avg_loss = 0.\n",
    "    optimizer.zero_grad()\n",
    "    if verbose:\n",
    "        bar = tqdm(total=len(train_loader))\n",
    "    mixup_fn = tu.Mixup(prob=MIXUP, switch_prob=0.0, onehot=True, label_smoothing=0.05, num_classes=CLASSES)\n",
    "    for idx, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs_train, labels_train = imgs.float().cuda(), labels.cuda()\n",
    "        if MIXUP:\n",
    "            imgs_train, labels_train = mixup_fn(imgs_train, labels_train)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output_train, _ = model_conv(imgs_train)\n",
    "            loss = criterion(output_train, labels_train)\n",
    "        scaler.scale(loss).backward()\n",
    "        if ((idx+1)%ACCUMULATE==0): # Gradient Accumulate\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        avg_loss += loss.item() / len(train_loader) \n",
    "        if verbose:\n",
    "            bar.update(1)\n",
    "    if verbose:\n",
    "        bar.close()\n",
    "    return avg_loss\n",
    "\n",
    "def test_model():    \n",
    "    avg_val_loss = 0.\n",
    "    model_conv.eval()\n",
    "    y_true_val = np.zeros(len(valset))\n",
    "    y_pred_val = np.zeros((len(valset), CLASSES))\n",
    "    with torch.no_grad():\n",
    "        for idx, (imgs, labels) in enumerate(val_loader):\n",
    "            imgs_vaild, labels_vaild = imgs.float().cuda(), labels.cuda()\n",
    "            output_test, _ = model_conv(imgs_vaild)\n",
    "            avg_val_loss += (criterion_test(output_test, labels_vaild).item() / len(val_loader)) \n",
    "            a = labels_vaild.detach().cpu().numpy().astype(np.int)\n",
    "            b = softmax(output_test.detach().cpu().numpy(), axis=1)\n",
    "\n",
    "            y_true_val[idx*BATCH_SIZE:idx*BATCH_SIZE+b.shape[0]] = a\n",
    "            y_pred_val[idx*BATCH_SIZE:idx*BATCH_SIZE+b.shape[0]] = b\n",
    "            \n",
    "    metric_val = sum(np.argmax(y_pred_val, axis=1) == y_true_val) / len(y_true_val)\n",
    "    return avg_val_loss, metric_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "placed-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold):\n",
    "    best_avg_loss = 100.0\n",
    "    best_acc = 0.0\n",
    "\n",
    "    avg_val_loss, avg_val_acc = test_model()\n",
    "    print('pretrain val loss %.4f precision %.4f'%(avg_val_loss, avg_val_acc))       \n",
    "\n",
    "    ### training\n",
    "    for epoch in range(EPOCH):   \n",
    "        print('lr:', optimizer.param_groups[0]['lr']) \n",
    "        np.random.seed(SEED+EPOCH*999)\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_model(epoch)\n",
    "        avg_val_loss, avg_val_acc = test_model()\n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t train_loss={:.4f} \\t val_loss={:.4f} \\t val_precision={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, EPOCH, avg_loss, avg_val_loss, avg_val_acc, elapsed_time))\n",
    "\n",
    "        if avg_val_loss < best_avg_loss:\n",
    "            best_avg_loss = avg_val_loss\n",
    "\n",
    "        if avg_val_acc > best_acc:\n",
    "            best_acc = avg_val_acc\n",
    "            torch.save(model_conv.module.state_dict(), '../exp/exp' + str(EXP) + '/model-best' + str(fold) + '.pth')\n",
    "            print('model saved!')\n",
    "\n",
    "        print('=================================')   \n",
    "\n",
    "    print('best loss:', best_avg_loss)\n",
    "    print('best precision:', best_acc)\n",
    "    return best_avg_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-contents",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Fold 0 **********\n",
      "\n",
      "pretrain val loss 9.4469 precision 0.0057\n",
      "lr: 0.0003\n"
     ]
    }
   ],
   "source": [
    "log = open('../exp/exp' + str(EXP) +'/log.txt', 'w')\n",
    "log.write('SEED%d\\n'%SEED)\n",
    "cv_losses = []\n",
    "cv_metrics = []\n",
    "\n",
    "for fold in range(FOLD):\n",
    "    print('\\n ********** Fold %d **********\\n'%fold)\n",
    "    ###################### Dataset #######################\n",
    "    labels = train_df.label.unique()\n",
    "    label_encoder = {}\n",
    "    for idx, name in enumerate(labels):\n",
    "        label_encoder.update({name:idx})\n",
    "    \n",
    "    trainset = LeavesDataset(train_df.iloc[tr_folds[fold]].reset_index(), label_encoder, base_dir, train_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True, drop_last=True, worker_init_fn=tu.tools.worker_init_fn)\n",
    "    \n",
    "    valset = LeavesDataset(train_df.iloc[val_folds[fold]].reset_index(), label_encoder, base_dir, test_transform)\n",
    "    val_loader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "    \n",
    "    ####################### Model ########################\n",
    "    model_conv = tu.ImageModel(name='resnest50d', pretrained=True, num_feature=2048, classes=CLASSES)\n",
    "    model_conv.cuda()\n",
    "    model_conv = torch.nn.DataParallel(model_conv)\n",
    "\n",
    "    ###################### Optim ########################\n",
    "    optimizer = tu.RangerLars(model_conv.parameters(), lr=LR, weight_decay=2e-4)\n",
    "\n",
    "    if MIXUP:\n",
    "        criterion = tu.SoftTargetCrossEntropy()\n",
    "    else:\n",
    "        criterion = tu.LabelSmoothingCrossEntropy()\n",
    "        \n",
    "    criterion_test = nn.CrossEntropyLoss()\n",
    "\n",
    "    T = len(train_loader)//ACCUMULATE * EPOCH # cycle\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=T, eta_min=LR/DECAY_SCALE)\n",
    "    \n",
    "    val_loss, val_acc = train(fold)\n",
    "    \n",
    "    cv_losses.append(val_loss)\n",
    "    cv_metrics.append(val_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "cv_loss = sum(cv_losses) / FOLD\n",
    "cv_acc = sum(cv_metrics) / FOLD\n",
    "print('CV loss:%.6f  CV precision:%.6f'%(cv_loss, cv_acc))\n",
    "log.write('CV loss:%.6f  CV precision:%.6f\\n\\n'%(cv_loss, cv_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2604b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.close()\n",
    "tu.tools.backup_folder('.', '../exp/exp%d/src'%EXP)  # backup code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}